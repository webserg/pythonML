{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function shows us how good our model is. In multy class recognizing we can get following numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = np.array([5,2,-1,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second key ingredient we need is a loss function, which is a differentiable objective that quantifies our unhappiness with the computed class scores. Intuitively, we want the correct class to have a higher score than the other classes. When this is the case, the loss should be low and otherwise the loss should be high. There are many ways to quantify this intuition, but in this example lets use the cross-entropy loss that is associated with the Softmax classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the Softmax classifier interprets every element of f as holding the (unnormalized) log probabilities of the three classes. We exponentiate these to get (unnormalized) probabilities, and then normalize them to get probabilites. Therefore, the expression inside the log is the normalized probability of the correct class. Note how this expression works: this quantity is always between 0 and 1. When the probability of the correct class is very small (near 0), the loss will go towards (positive) infinity. Conversely, when the correct class probability goes towards 1, the loss will go towards zero because log(1)=0. Hence, the expression for Li is low when the correct class probability is high, and itâ€™s very high when it is low.\n",
    "http://cs231n.github.io/neural-networks-case-study/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11494422, 0.09975654, 0.28504199, 0.34777442, 0.15496837],\n",
       "       [0.98433883, 0.46763162, 0.14608097, 0.39536274, 0.95039889],\n",
       "       [0.32166425, 0.33591235, 0.76139172, 0.88183236, 0.28131679]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.rand(3, 5)\n",
    "scores = np.array([0,1,4])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "expretion gives us probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.18270343, 0.17994956, 0.21658044, 0.23060227, 0.1901643 ],\n",
       "       [0.2816529 , 0.16800072, 0.12180454, 0.15628783, 0.27225402],\n",
       "       [0.15928496, 0.16157071, 0.24725551, 0.27890268, 0.15298614]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_examples = X.shape[0]\n",
    "# get unnormalized probabilities\n",
    "exp_scores = np.exp(X)\n",
    "# normalize them for each example\n",
    "probs = exp_scores / np.sum(exp_scores, axis=1,keepdims=True)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_logprobs = -np.log(probs[range(num_examples),scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7870286679351468"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loss = np.sum(correct_logprobs)/num_examples\n",
    "data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "    sum = np.sum(x)\n",
    "    return x / sum\n",
    "\n",
    "\n",
    "def softmax(W):\n",
    "    Wexp = np.exp(W)\n",
    "    print(Wexp)\n",
    "    print(norm(Wexp))\n",
    "    log = -np.log10(norm(Wexp))\n",
    "    print(log)\n",
    "    print(np.max(log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss_vectorized(W, X, y, reg):\n",
    "  \"\"\"\n",
    "  Softmax loss function, vectorized version.\n",
    "\n",
    "  Inputs and outputs are the same as softmax_loss_naive.\n",
    "  \"\"\"\n",
    "  # Initialize the loss and gradient to zero.\n",
    "  loss = 0.0\n",
    "  dW = np.zeros_like(W)\n",
    "  num_classes = W.shape[1]\n",
    "  num_train = X.shape[0]\n",
    "\n",
    "  #############################################################################\n",
    "  # TODO: Compute the softmax loss and its gradient using no explicit loops.  #\n",
    "  # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
    "  # here, it is easy to run into numeric instability. Don't forget the        #\n",
    "  # regularization!                                                           #\n",
    "  #############################################################################\n",
    "\n",
    "  # loss\n",
    "  # score: N by C matrix containing class scores\n",
    "  scores = X.dot(W)\n",
    "  scores -= scores.max()\n",
    "  scores = np.exp(scores)\n",
    "  scores_sums = np.sum(scores, axis=1)\n",
    "  cors = scores[range(num_train), y]\n",
    "  loss = cors / scores_sums\n",
    "  loss = np.sum(np.log(loss))/num_train + reg * np.sum(W * W) * -1\n",
    "\n",
    "  # grad\n",
    "  s = np.divide(scores, scores_sums.reshape(num_train, 1))\n",
    "  s[range(num_train), y] = - (scores_sums - cors) / scores_sums\n",
    "  dW = X.T.dot(s)\n",
    "  dW /= num_train\n",
    "  dW += 2 * reg * W\n",
    "\n",
    "  #############################################################################\n",
    "  #                          END OF YOUR CODE                                 #\n",
    "  #############################################################################\n",
    "\n",
    "  return loss, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in pytorch for loss uses CrossEntropyLoss but usually it split in two function m = nn.LogSoftmax() loss = nn.NLLLoss() because log softmax can be count like last layer in model, and if take in count that model countet in GPU that it is faster than just use CrossEntropyLoss countable in CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"def softmax(x):\\n\",\n",
    "    \"    return torch.exp(x)/torch.sum(torch.exp(x), dim=1).view(-1, 1)\\n\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myCrossEntropyLoss(outputs, labels):\n",
    "    batch_size = outputs.size()[0]\n",
    "    # batch_size\n",
    "    tmp_outputs = F.softmax(outputs, dim=1)\n",
    "    print(tmp_outputs)# compute the log of softmax values\n",
    "    outputs = F.log_softmax(outputs, dim=1)\n",
    "    print(outputs)# compute the log of softmax values\n",
    "    outputs = outputs[range(batch_size), labels] # pick the values corresponding to the labels\n",
    "    return -torch.sum(outputs)/len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9763,  0.3201, -0.8598,  1.6141, -1.6732],\n",
      "        [-0.9189,  0.5164,  1.0250, -0.6426,  0.8590],\n",
      "        [ 0.7105, -0.5550,  0.0271, -0.7494, -0.1751]])\n",
      "3\n",
      "tensor(2.3586)\n",
      "tensor([[0.5072, 0.0968, 0.0297, 0.3531, 0.0132],\n",
      "        [0.0515, 0.2163, 0.3597, 0.0679, 0.3047],\n",
      "        [0.4112, 0.1160, 0.2076, 0.0955, 0.1696]])\n",
      "tensor([[-0.6789, -2.3351, -3.5150, -1.0411, -4.3283],\n",
      "        [-2.9664, -1.5311, -1.0225, -2.6901, -1.1885],\n",
      "        [-0.8886, -2.1542, -1.5720, -2.3485, -1.7742]])\n",
      "tensor(2.3586)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\webse\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:9: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "m = nn.LogSoftmax()\n",
    "loss = nn.NLLLoss()\n",
    "# input is of size N x C = 3 x 5\n",
    "input = torch.randn(3, 5)\n",
    "print(input)\n",
    "# each element in target has to have 0 <= value < C\n",
    "target = torch.tensor([1, 0, 4])\n",
    "print(len(target))\n",
    "output = loss(m(input), target)\n",
    "print(output)\n",
    "output2 = myCrossEntropyLoss(input, target)\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
